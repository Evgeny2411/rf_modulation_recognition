{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LeakyReLU, Dense, Conv1D,Flatten, Dropout, AveragePooling1D, Input, concatenate, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"2018.01/permutated_data.h5\"\n",
    "BUFFER_SIZE = 20000\n",
    "SAMPLE_SIZE = 851968\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_SIZE = int(0.9 * SAMPLE_SIZE)\n",
    "TEST_SIZE = int(0.05 * SAMPLE_SIZE)\n",
    "VAL_SIZE = int(0.05 * SAMPLE_SIZE)\n",
    "NUM_CLASSES = 8\n",
    "SIGNAL_SIZE = 1024\n",
    "LEAKY_RELU_ALPHA = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_dataset(dataset, train_size, test_size, validation_size, batch_size):\n",
    "    '''\n",
    "    splits a tensorflow dataset into training, validation and test sets\n",
    "    '''\n",
    "    train_dataset = dataset.take(train_size//batch_size)\n",
    "    val_dataset = dataset.skip(train_size//batch_size).take(validation_size//batch_size)\n",
    "    test_dataset = dataset.skip(train_size//batch_size + validation_size//batch_size).take(test_size//batch_size)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def load_data_from_hdf5_amplitude_phase(file_path):\n",
    "    '''\n",
    "    loads dataset from hdf5 file and returns a tensorflow datasets splitted\n",
    "    '''\n",
    "    def generator():\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            for i in range(0, SAMPLE_SIZE, BATCH_SIZE):\n",
    "                batch = f['X'][i:i+BATCH_SIZE]\n",
    "                batch = tf.cast(batch, tf.float32)\n",
    "                batch = (batch - tf.reduce_min(batch)) / (tf.reduce_max(batch) - tf.reduce_min(batch))\n",
    "                i_data = f['X'][i:i+BATCH_SIZE, :, 0]\n",
    "                q_data = f['X'][i:i+BATCH_SIZE, :, 1]\n",
    "                complex_data = i_data + 1j*q_data\n",
    "                \n",
    "                amplitude = np.abs(complex_data)\n",
    "                phase = np.angle(complex_data)\n",
    "\n",
    "                amp_phase_batch = np.stack((amplitude, phase), axis=-1)        \n",
    "            \n",
    "                labels = f['Y'][i:i+BATCH_SIZE, :NUM_CLASSES]\n",
    "                snr = f['Z'][i:i+BATCH_SIZE]\n",
    "            \n",
    "                yield batch, amp_phase_batch, labels, snr\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(generator, output_signature=(\n",
    "    tf.TensorSpec(shape=(BATCH_SIZE, SIGNAL_SIZE, 2), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(BATCH_SIZE, SIGNAL_SIZE, 2), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(BATCH_SIZE, NUM_CLASSES), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(BATCH_SIZE, 1), dtype=tf.float32)\n",
    "    ))\n",
    "    \n",
    "    return split_dataset(dataset, TRAIN_SIZE, TEST_SIZE, VAL_SIZE, BATCH_SIZE)\n",
    "\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def map_dataset(signal, mod, label, snr):\n",
    "    return ((signal, mod), label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL COMPILING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(\n",
    "    \"logs\",\n",
    "    \"fit\",\n",
    "    datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    ")\n",
    "# INPUTS\n",
    "input_iq = Input(shape=(SIGNAL_SIZE, 2))\n",
    "input_ap = Input(shape=(SIGNAL_SIZE, 2)) \n",
    "\n",
    "# I/Q BLOCK\n",
    "\n",
    "x = Conv1D(64, 5, kernel_regularizer=l2(0.001))(input_iq)\n",
    "x = LeakyReLU(alpha=LEAKY_RELU_ALPHA)(x)\n",
    "x = AveragePooling1D(2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv1D(128, 3, kernel_regularizer=l2(0.001))(x)\n",
    "x = LeakyReLU(alpha=LEAKY_RELU_ALPHA)(x)\n",
    "x = AveragePooling1D(2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv1D(128, 3, kernel_regularizer=l2(0.001))(x)\n",
    "x = LeakyReLU(alpha=LEAKY_RELU_ALPHA)(x)\n",
    "x = AveragePooling1D(2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv1D(256, 3, kernel_regularizer=l2(0.001))(x)\n",
    "x = LeakyReLU(alpha=LEAKY_RELU_ALPHA)(x)\n",
    "x = AveragePooling1D(2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv1D(512, 3, kernel_regularizer=l2(0.001))(x)\n",
    "x = LeakyReLU(alpha=LEAKY_RELU_ALPHA)(x)\n",
    "x = AveragePooling1D(2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Model(inputs=input_iq, outputs=x)\n",
    "\n",
    "# AMDLITUDE/PHASE BLOCK\n",
    "\n",
    "y = Conv1D(64, 5, kernel_regularizer=l2(0.001))(input_ap)\n",
    "y = LeakyReLU(alpha=LEAKY_RELU_ALPHA)(y)\n",
    "y = AveragePooling1D(2)(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Dropout(0.2)(y)\n",
    "\n",
    "y = Conv1D(128, 3, kernel_regularizer=l2(0.001))(y)\n",
    "y = LeakyReLU(alpha=LEAKY_RELU_ALPHA)(y)\n",
    "y = AveragePooling1D(2)(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Dropout(0.2)(y)\n",
    "\n",
    "y = Conv1D(128, 3, kernel_regularizer=l2(0.001))(y)\n",
    "y = LeakyReLU(alpha=LEAKY_RELU_ALPHA)(y)\n",
    "y = AveragePooling1D(2)(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Dropout(0.2)(y)\n",
    "\n",
    "y = Conv1D(256, 3, kernel_regularizer=l2(0.001))(y)\n",
    "y = LeakyReLU(alpha=LEAKY_RELU_ALPHA)(y)\n",
    "y = AveragePooling1D(2)(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Dropout(0.2)(y)\n",
    "\n",
    "y = Conv1D(512, 3, kernel_regularizer=l2(0.001))(y)\n",
    "y = LeakyReLU(alpha=LEAKY_RELU_ALPHA)(y)\n",
    "y = AveragePooling1D(2)(y)\n",
    "y = BatchNormalization()(y)\n",
    "\n",
    "y = Flatten()(y)\n",
    "y = Model(inputs=input_ap, outputs=y)\n",
    "\n",
    "# CONCAT BLOCKS\n",
    "combined = concatenate([x.output, y.output])\n",
    "\n",
    "# DENSE BLOCK\n",
    "z = Dense(256, kernel_regularizer=l2(0.001))(combined)\n",
    "z = LeakyReLU(alpha=LEAKY_RELU_ALPHA)(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = Dropout(0.2)(z)\n",
    "\n",
    "z = Dense(128, kernel_regularizer=l2(0.001))(z)\n",
    "z = LeakyReLU(alpha=LEAKY_RELU_ALPHA)(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = Dropout(0.2)(z)\n",
    "\n",
    "z = Dense(64, kernel_regularizer=l2(0.001))(z)\n",
    "z = LeakyReLU(alpha=LEAKY_RELU_ALPHA)(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = Dropout(0.2)(z)\n",
    "\n",
    "z = Dense(32, kernel_regularizer=l2(0.001))(z)\n",
    "z = LeakyReLU(alpha=LEAKY_RELU_ALPHA)(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = Dropout(0.2)(z)\n",
    "\n",
    "# OUTPUT\n",
    "z = Dense(NUM_CLASSES, activation=\"softmax\")(z)\n",
    "\n",
    "model = Model(inputs=[x.input, y.input], outputs=z)\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "checkpoint_callback = ModelCheckpoint(filepath='iq_AP_cnn_50.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, min_delta=0.03)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset, val_dataset, test_dataset = load_data_from_hdf5_amplitude_phase(FILE_PATH)\n",
    "\n",
    "history = model.fit(train_dataset.map(map_dataset), validation_data=val_dataset.map(map_dataset), epochs=50,  callbacks=[tensorboard_callback, checkpoint_callback, early_stopping_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
